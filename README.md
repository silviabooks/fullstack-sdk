# FullStack-SDK

This repository collects a few components that help you build a modern Web Application with the following core values in mind:

- The Developer Experience matters
- The Development Environment is automated
- The Development Environment is stateless
- The Development Environment is reproducible
- We follow Test Driven Development principles
- We maximize parallel work streams

## Start Coding

The following commands have been tested on MacOSX, Windows running WSL2 with Docker and on GitPod.  
You need `Docker` to be up and running on your system.

```bash
# Start & Stop
make start
make stop

# Full State Reset
# (it will also rebuild all the containers)
make reset
```

\*Note: If you encounter failure when running the `make start` , ensure that you have disabled `buildkit` option. Which is located inside Docker Desktop application -> Settings -> Docker Engine and set `"buildkit": true` to `"buildkit": false` .

or click this button to enjoy a **Remote Development Environment**:

[![Open in GitPod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io#https://github.com/revisohq/fullstack-sdk)

## Table of Contents

- [Project's Composition](#projects-composition)
- [Automated Development Environment](#automated-development-environment)
- [Docker & DockerCompose](#docker--dockercompose)
  - [Types of Containers](#types-of-containers)
  - [Dependencies Between Services](#dependencies-between-services)
  - [Ports Mapping](#ports-mapping)
  - [Volumes Mapping](#volumes-mapping)
  - [Environmental Variables & Secret](#environmental-variables--secrets)
- [The Make Interface](#the-make-interface)
  - [make start](#make-start)
  - [make stop](#make-stop)
  - [make test](#make-test)
  - [make restart](#make-restart)
  - [make reset](#make-reset)
  - [make build](#make-build)
  - [make clean](#make-clean)
- [PostgreSQL](#postgresql)
  - [Schema Management & Migrations](#schema-management--migrations)
  - [Serverless Functions with TDD](#serverless-functions)
  - [TDD on PostgreSQL](#tdd-on-postgresql)
- [Hasura.io](#hasuraio)
  - [The Architectural Role of Hasura](#the-architectural-role-of-hasura)
  - [Migrations & State Management](#migrations--state-management)
  - [Point & Click Configuration](#point--click-configuration)
- [JSON Web Tokens - JWT.io](#json-web-tokens)
- [JavaScript](#javascript)
  - [ForrestJS](#forrestjs)
  - [Axios](#axios)
  - [Jsonebtoken](#jsonwebtoken)
  - [Fastify](#fastify)
  - [React](#react) \*
- [TDD - Test Driven Development](#tdd---test-driven-development)
  - [Pair Programming & TDD Cycle](#pair-programming--tdd-cycle)
- [State Management Utilities](#state-management-utilities)
  - [make hasura-console](#make-hasura-console)
  - [make hasura-apply](#make-hasura-apply)
  - [make hasura-export](#make-hasura-export)
- [Implicit Migrations](#implicit-migrations)
  - [Idempotency is Key](#idempotency-is-key)
  - [Migrations are Immutable](#migrations-are-immutable)
- [AuthenticationLESS Apps](#authenticationless-apps)
- [NodeJS Backend App](#nodejs-backend-app)
  - [Files Structure](#files-structure)
  - [Make & DockerCompose Interface](#make--dockercompose-interface)
  - [NPM Interface](#npm-interface)
  - [TDD & Native Tests](#tdd--native-tests)
- [Client App in React](#client-app-in-react)
  - [WebApp vs SPA](#webapp-vs-spa)
  - [CRA](#cra)
  - [TS Libraries & Components](#ts-libraries--components)
  - [Service Calls](#service-calls)
  - [Webpack Proxy](#webpack-proxy)
  - [Client Routing](#client-routing)

---

## Project's Composition

### Services

Services are stuff that in a Production environment we will likely purchase from an external supplier.

- [postgres](./services/postgres/README.md)
- [hasura-engine](./services/hasura-engine/README.md)
- [hasura-console](./services/hasura-console/README.md)
- [migrations](./services/migrations/README.md)
- [adminer](https://www.adminer.org/)

### Apps

Apps are stuff that we build. Our code. Our precious.

- [auth](./apps/auth/README.md)

---

## Automated Development Environment

Striving to improve the Developer Experience to the exteme, this Project provides a high degree of automation:

1. It can run in your browser thanks to [GitPod.io](https://gitpod.io)
2. It builds and run locally thanks to [DockerCompose](https://docs.docker.com/compose/)

We will go in great details regarding the DockerCompose setup, and how to run the system in your local machine.

ðŸ”¥ But the easiest way to run the project is in a [GitPod Workspace](https://www.gitpod.io/docs/life-of-workspace) ðŸ”¥

[![Open in GitPod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io#https://github.com/revisohq/fullstack-sdk)

This is made possible by Kuberneted and two small files:

- `.gitpod.yml`
- `.gitpod.Dockerfile`

Without going in much details, you can edit those files to configure your workspace with any kind of sotware that you may think to use on a Linux machine, and any VSCode Extension or configuration that you are used to.

ðŸ¤¬ I don't get any money if you yse GitPod or not... I simply believe is a great tool and sponsor it in a transparent way ðŸ¤Ÿ.

---

## Docker & DockerCompose

The Development Environment is a composition of **containerized services**. Each service should expose a `Dockerfile` for development and production that is used by a `docker-compose.yml`.

Dependencies resolutions, building steps, development servers, etc **SHOULD BE ABSTRACTED AWAY** from the developer.

Docker & DockerCompose should be the only strict dependency on the Developer's machine. (they are free on Linux)

### Types of Containers

The `docker-compose.yml` for development should be divided in four sections:

- APPS: code that is created and maintained by our Engineers
- TESTS: run containerized tests agains the Apps
- UTILITIES: stuff like Adminer or similar that would not run in production
- SERVICES: anything that we would buy in production  
  <small>(Databases, Redis, ...)</small>

### Dependencies Between Services

We rely on the `depends_on` and `healthcheck` declarations to create a **boot ordered tree** that will reliably boot and instrument the entire Development Environment right after the first `git clone`.

> This is intended as a development facilitation and should have no impact in production.

EXPLANATION:

A **PRODUCTION** environment implements an **eager crash** approach where a service that fails a precondition (eg. the DB is not ready) will simply crash. The runner (Kube? Docker?) will apply a failover policy and restart it.

In a **DEVELOPMENT** environment, the service is likely wrapped by a [File System Monitoring Mechanism](https://www.npmjs.com/package/nodemon) that will rebuild/restart the service any time its source code chages.

In case of a "genuine - bug driven - crash", such event will be catched by the FS Monitor and not by the runner.

> Hence, in development we must be sure that the service's infrastructural preconditions are met BEFORE attempting to start a service.

### Ports Mapping

All the ports that need to be exposed to the host machine should be defaulted using the host's environmental variables:

```
postgres:
  ports:
    - "${POSTGRES_PORT:-5432}:5432"
```

### Volumes Mapping

[[TO BE COMPLETED]]

### Environmental Variables & Secrets

[[TO BE COMPLETED]]

---

## The Make Interface

We aim to simplify the Developer Experience by suggestig a seamless way to interact with any project which is based on a `Makefile`, hence native to virtually any Linux distributions.

### make start

- build Development Containers
- pulls project's dependencies
- boot the entire Development Environment
- prepare the initial state
- show relevant logs

### make stop

- gracefully tears down the Development Environment

### make restart

[Have you tried to turn it off, then on again?](https://www.youtube.com/watch?v=nn2FB1P_Mn8)

- stop
- start

### make reset

Like `make restart`, but it also clean up the state and rebuild any dependency / containers.

ðŸ‘‰ It's a fresh start over.

### make build

- build Development Containers ignoring any cache layer
- pulls project's dependencies

### make clean

- stop the system
- removes any kind of local state

### make test

- run all Unit Tests
- reset or prepare the Application State for the tests
- run all E2E Tests

---

## PostgreSQL

PostgreSQL is an Open Source [DBMS](https://en.wikipedia.org/wiki/Database) made by the Aliens to prove their mental superiority. They did it.

PostgreSQL can:

- handle relational data
- handle binary JSON documents
- handle geospatial data
- run server side functions in multiple languages
- provide additional features via extensions
- partition tables boosting big-data performances
- route tables to different disks
- handle push notifications
- **PERFORM UNIT TESTS AND TDD ON POSTGRES**

With the amount of data that we (as Industry) manage in an average size project / service we could use only PG as data management solution and live happily ever after.

_There are of course exceptions to this bold statement._

Most engineers go through life learning a little bit of many things. My challenge for you is to become a deep expert on this powerful tool and see where it take us.

SUGGESTED MATERIALS:

- [PostgreSQL Bootcamp: Go From Beginner to Advanced, 60+hours](https://www.udemy.com/course/postgresqlmasterclass/)
- [AmazingPostgres: a personal notebook](https://github.com/marcopeg/amazing-postgresql)
- [Tips for Tuning PostgreSQL 12 like a Pro | Learn how to prevent Postgres performance problems](https://www.udemy.com/course/postgresql-high-performance-tuning-guide/)

### Schema Management & Migrations

Please refer to the [State Management & Migrations](#state-management--migrations) section, and also have a read to the [Implicit Migrations](#implicit-migrations) proposal.

### Serverless Functions

**Serverless Functions** seem to be a cool new tech.

Or is it?

You hand over a piece of logic - possibly stateless logic - and ask a third party entity to run it for you in connection with API events or Data Events. Simple.

[ðŸ‘‰ Well, it's about 40 years that we have this tech.](https://lostechies.com/chadmyers/2010/09/08/a-brief-history-of-programming/)  
**We - as Industry - simply forgot about it.**

Since the Â´80s, it is possible to deploy serverless functions into most _DBMS_ such as MSSQL, MySQL, and of course, PostgreSQL.

> The **main argument against** this practice is the difficulty to debug and maintain some logic that becomes part of the state of the App.

But somehow, today we do exactly that with pieces of code (Node, .NET, Java) that we persist into Cloud Provider's state and trust them to execute it for us.

ðŸ‘‰ SO IT SEEMS WE OVERCAME THAT HICCUPS TOWARDS SERVERLESS ðŸ‘ˆ

There are two tools that build confidence with Serverless:

- versioning
- testing

When it comes to Serverless on PostgreSQL we can leverage those tools:

- [State Management & Migrations](#state-management--migrations)
- [Unit Testing on PostgreSQL](#tdd-on-postgresql)

Logical reliability is NOT a problem - and it's never been!

**ðŸ‘‰ SCALABILITY IS ANOTHER IMPORTANT SUBJECT TO DISCUSS ðŸ‘ˆ**

When we put logic on the Application Layer (aka: Servers or Cloud provided Serverless Functions) we can leverage on **HORIZONTAL SCALABILITY**.

> When we deploy Serverless Functions in a DBMS we simply CAN NOT SCALE HORIZONTALLY. Period.

Is it a problem?  
Maybe yes, maybe no.

Up until approximately 10 years (~2010s), **VERTICAL SCALING WAS BOTH DIFFICULT AND LIMITED**. Not to mention expensive.

It wasn't such a good idea to keep scaling up a DBMS Server. Too much work. And the [VERTICAL LIMIT wasn't as high as K2](https://www.imdb.com/title/tt0190865/).

---

But then...

> Take me to the magic of the moment  
> On a glory night  
> Where the children of tomorrow share their dreams
>
> [The Wind of Change by Scorpions](https://www.youtube.com/watch?v=n4RjJKxsamQ)

---

Today (~2020s), the vertical limit is high. Very high. Changing virtual hardware (CPU, Memory, Disks) is a highly automated procedure that we delegate to Terraform or similar infra-as-code tools.

A single (or replicated) DBMS machine has the possibility to scale up and match requirements that are simply mind-blowing.

**ðŸ‘‰ TODAY WE CAN MOVE OUR LOGIC (BACK) TO WHERE IT BELONGS ðŸ‘ˆ**

After all, business logic has born into DBMS (AS400), then it moved out for economical reasons.

It's time to move it back, and make a better usage of the savings!

**NOT EVERYTHING BELONGS TO THE DBMS**

Of course, we have a SOlid (pun inteded) responsibility in choosing what belongs to the DBMS and what not:

- âœ… joining related data definitely belongs to the DBMS.
- ðŸš« rendering a PDF definitely doesn't belong to he DBMS.
- ðŸš§ validating a login and releasing a JWT? Maybe.

One thing is certain: the only real argument agains Database Serverless was MONEY, and it is not a valid argument today.

### TDD on PostgreSQL

As we move logic into the Database Layer we must keep reliability up. Luckily, there is no easier place to do so.

[PGTap](https://pgtap.org/) is a testing framework for Postgres that is:

- easy to learn and use
- [can run in Docker](https://github.com/marcopeg/amazing-postgresql/tree/main/testing/unit-tests)
- can be part of our CI/CD

We can achieve 100% code-coverage for all the business critical data centric logic that we write.

ðŸ‘‰ **PLUS:** Tests run in transactions so they are **STATELESS BY DESIGN!** ðŸ‘ˆ

---

## Hasura.io

[Hasura.io](https://hasura.io/) is a super-charged ORM-like layer that **maps a database schema to a GraphQL API**. And much more.

> Do you remember all the code we used to write to expose APIs, validate tokens, match authorization claims and sanitize parameters?
>
> It's all gone with the wind.

Hasura Engine can:

- map multiple PostgreSQL or MSSQL databases and schemas to a GraphQL APIs effectively federating data access
- manage authentication / authorization via JWT claims or custom authentication services
- provide **fine-grained data access control** implementing declarative rules
- manage live data subscriptions via sockets (and fallback)
- provide a GraphQL API proxy to custom backend services effectively providing **static data-type validation** on REST APIs input/output
- federate multiple custom backends into a single self-documented GraphQL API
- federate remote GraphQL schemas
- manage side effects by forwarding data events to custom backends with safe failover strategies
- manage cron-jobs
- manage migrations

Suggested materials:

- [Performant GraphQL Backend in 1 Day by Using Hasura Engine](https://www.udemy.com/course/develop-graphql-backend-faster-with-hasura/)
- [Getting started docs](https://hasura.io/docs/latest/graphql/core/index.html)
- [The Hasura CLI](https://hasura.io/docs/latest/graphql/core/hasura-cli/index.html)
- [Migrations & Metadata](https://hasura.io/docs/latest/graphql/core/migrations/index.html)
- [Hasura License (Apache2)](https://github.com/hasura/graphql-engine/blob/master/LICENSE)

### The Architectural Role of Hasura

We can use Hasura as our **backend-for-frontend** as so unify how our React App makes server calls.

Most of the **CRUD operations will be described as Hasura schema based API calls**, effectively removing (or postponing) to write boring and error-prone code that is costly to write, test and maintain.

We can **proxy any custom service** through Hasura with simple declarative rules that can be automatically propagated to any environment.

We can **describe maintenance jobs** as Hasura Events and keep the knowledge of those stuff in the Git codebase along with any custom logic.

> Hasura's medatata is Git-controlled.

### Migrations & State Management

Using Hasura.io as backend-for-frontend implies that the Application State is composed by:

1. database(s) schema(s)
2. Hasura(s)' metadata
3. data seeding scripts
4. data snapshots

Points n 1, 2 and 3 are managed by the [Hasura CLI](https://hasura.io/docs/latest/graphql/core/migrations/index.html#how-is-hasura-state-managed) via:

- migrations
- metadata
- seeding

Point n.4 is slightly more complicated and is usually delegated to the DevOps management realm. Long story short, the full state should be restored to a newly supplied environment. The classic name for such a huge responsibility is **Disaster Recovery**.

ðŸ”¥ You can run this on your host machine after [installing the Hasura CLI](https://hasura.io/docs/latest/graphql/core/hasura-cli/install-hasura-cli.html):

```bash
cd services/migrations
hasura apply migrations
hasura apply metadata
hasura apply seeds
```

ðŸ”¥ [Or you can use the dockerized state management utilities](#state-management-utilities) ðŸ¤Ÿ

### Point & Click Configuration

Hasura.io offers a visual management tool called [Hasura Console](https://hasura.io/docs/latest/graphql/core/hasura-cli/hasura_console.html) that can **synchronize point&click actions with the local codebase** for metadata and migrations.

1. our Engineers can operate the Declarative ACL rules, Proxy Actions, setup side-effects and Events using the Visual Console
2. the resulting State Mutation is recorded as [database migrations](#migrations--state-management) and code-changes in a set of YAML files
3. the Team can support the change by offering Code Reviews and testing the branch in [dispsable environments](#disposable-environments--gitpodio)
4. approved changes are automatically distributed to any running environment including the Developer's

> ðŸ”¥ You can run this on your host machine after [installing the Hasura CLI](https://hasura.io/docs/latest/graphql/core/hasura-cli/install-hasura-cli.html):

```bash
cd services/migrations
hasura console
```

ðŸ”¥ [Or you can use the dockerized state management utilities](#state-management-utilities) ðŸ¤Ÿ

---

## JSON Web Tokens

JSON Web Tokens are an open, industry standard [RFC 7519](https://tools.ietf.org/html/rfc7519) method for representing claims securely between two parties.

ðŸ‘‰ [JWT.IO allows you to decode, verify and generate JWT](https://jwt.io)

We use JWT

---

## Javascript

Javascript is the language of the web, and the **ONLY REAL FULL-STACK LANGUAGE** available to mankind because it's the only one that runs ALSO on a browser.

> Yes, there are sub-languages like [Typescript](https://www.typescriptlang.org/) that [transpiles](https://en.wikipedia.org/wiki/Source-to-source_compiler) to Javascript and provide features like strict types checking and look more like Object-Oriented languages that many are accustomed to.

**IMHO transpiling is an expensive operation** that slow down the Developer Experience. When you follow [a few Javascript bestpractices](https://marcopeg.com/javascript-the-good-parts-in-2020/), you don't have to worry about performances as the [mighty V8](https://v8.dev/) will kick in with all [its power and fury](https://www.digitalocean.com/community/tutorials/js-v8-engine).

### ForrestJS

The JavaScript Plugin Library

ðŸ‘‰ [Go to docs](https://forrestjs.github.io)

### Fastify

Fast and low overhead web framework, for Node.js

ðŸ‘‰ [Go to docs](https://www.fastify.io/)

### Axios

Promise based HTTP client for the browser and node.js

ðŸ‘‰ [Go to docs](https://axios-http.com/)

### Jsonwebtoken

An implementation of [JSON Web Tokens](https://tools.ietf.org/html/rfc7519).

ðŸ‘‰ [Go to docs](https://github.com/auth0/node-jsonwebtoken)

---

## TDD - Test Driven Development

We should strive to use Test Driven Development (TDD) and Pair Programming as the simple way to avoid the most common pitfalls of our Industry:

- **THE LOCAL STATE**
  - interpreter version
  - locally installed CLI utilities
  - data that was created manually
- external dependencies ([Impure Functions](https://dev.to/sanspanic/pure-vs-impure-functions-50aj))
- poor naming for variables and functions
- convoluted logic
- convoluted folder structure
- misspelling names

All these are **UNNECESSARY EVILS** that must be fought and beaten at any cost. Behind the luring speed of "just get coding" we hide the **immense maintenance costs of most legacy projects**.

### Pair Programming & TDD Cycle

TDD produces the best results when performed in [Pair Programming](https://en.wikipedia.org/wiki/Pair_programming) (PP), and PP produces the best Team results when the participants to a session change frequently and are at **DIFFERENT SENIORITY LEVELS**.

The Cycle between Jack and Jane should look like this:

![tdd-session-in-pair-programming-jack](./docs/diagrams/tdd-session-in-pair-programming-jack.svg)

Then both Jack and Jane take a coffe...

... and get back to work:

![tdd-session-in-pair-programming-jane](./docs/diagrams/tdd-session-in-pair-programming-jane.svg)

It is clear that Jack an Jane keep on **ALTERNATING ROLES** during a TDD Session. This practice **FORCES THEM TO COMMUNICATE** and explaining with words each others' ideas. It leads to better choiches and real-time code review.

I'll repeat: this process yields the best results when **PAIRING DIFFERENT SENIORITY LEVELS**.

**ðŸ”¥ THE ALTERNATION OF THE ROLES MAKES PAIR PROGRAMMING A WINNER PRACTICE ðŸ”¥**

### Test VS It Should

Most test suite provide the basic API:

- describe
- test / it

Where `test()` and `it()` are interchangable synonyms. So why bother with choosing one over the other?

Consider the following example:

```js
describe("My Sum function");
test("That it should return A + B");
test("That throws if given the wrong arguments");

describe("My Sum function");
it("Should return A + B");
describe("arguments");
it("Should throw when given the wrong values");
```

By using the form `it()` we have no choice but referring to the subject that is provided by the nearest `describe()`. It forces us to **SCOPE OUR TEST CASES** by combining multiple `describe()` blocks, enhancing clarity, SRP, and test segregation.

### Tests Drive Development

Not the other way around.

When you start writing a test for a piece of App that does not exists yet you are forced to focus on its API.

It doesn't matter if you are performing a Unit Test or an End-to-End test, you will ha

---

## State Management Utilities

This project setup offers a few utilities that you can use via `docker-compose` and the relative `make` interface.

### make hasura-console

It runs a conteinerized version of the [Hasura Console](https://hasura.io/docs/latest/graphql/core/hasura-cli/hasura_console.html) that will be available at port `9695` and `9693`.

> By default it is configured to apply (at boot time):
>
> - migrations
> - metadata
> - seeds.

ðŸ‘‰ **Use this console to keep your local metadata in sync with your clickings.** ðŸ‘ˆ

> Use the console at:  
> http://localhost:9695

### make hasura-apply

It runs a coneinerized version of the `hasura ** apply` command.

> By default it is configured to apply:
>
> - migrations
> - metadata
> - seeds.

### make hasura-export

It runs a coneinerized version of the `hasura ** export` command.

> By default it is configured to export only metadata, but you can also use it to generate a full initial migration.

---

## AuthenticationLESS Apps

All the Apps that we build should be **AUTHENTICATION-LESS** meaning that they should receive and persist an existing session that is maintained and secured by a single Authentication Authority.

A very simple technique to achieve acceptable security is [described by Auth0](https://auth0.com/blog/refresh-tokens-what-are-they-and-when-to-use-them/#:~:text=from%20compromised%20tokens.-,Refresh%20Token%20Automatic%20Reuse%20Detection,-Refresh%20tokens%20are) and shown in the following chart.

![AuthenticationLESS Flow](./docs/diagrams/authentication-less-flow.svg)

> The trick is that the first Refresh Token is **intended for immediate use** by the client App. It should have a lifespan of a few seconds, maybe a minute.
>
> We call it [**DELEGATION TOKEN**](./apps/auth/README.md#the-delegation-token).

This information can be safely forwarded via URI param as a malicious interceptor would:

1. have a very short lifespan to produce an attak
2. any racing condition with the legit/malicious user would invalidate the entire session

ðŸ‘‰ For a realistic damage to take place, a malicious attacker would have to **refresh the token BEFORE the legitimate bearer** makes her first attempt. In such a case, the attacker would have a valid Access Token in his hands. And that should be short-lived anyway.

ðŸ’¡ This event could be **FURTHERLY MITIGATED** by delaying the First Refresh giving time for a racing condition to take place BEFORE releasing the Access Token.

It would make for a slower First Page Loading Time, but would greatly increase security. With such combination, the only possible way to breach would be in case of a full hijacking of the redirect. Basically a bad guy sitting in front of an authenticated browser. But that's beyond service-to-service security.

---

## Implicit Migrations

We usually deal with **schema maintenance** by applying migrations to a running database.

Hasura has [its own migration utility](https://hasura.io/docs/latest/graphql/core/migrations/index.html) that facilitates distributing migrations (and seeds) to multiple databases at once, or independently.

> But when we deal with **REAL MICRO-SERVICEs** that handles a very small piece of state, such state become usually very stable.
>
> ðŸ‘‰ As it is in the case of the Auth Service in this Project.

In this case, I suggest to use **IMPLICIT MIGRATIONS** that is running **ðŸ”¥ IDEMPOTENT SCHEMA INSTRUCTIONS ðŸ”¥** at the service's boot.

This approach guarantee that every service is responsible for applying its own schema and seed data every time it gets deployed.

### Idempotency is Key

When running idempotent instructions, it doesn't really matter if multiple services will race at boot time. Idempotency will guarantee that structures and data are built only once.

When it comes to PostgreSQL we can leverage modifiers as in `IF NOT EXISTS` or `ON CONFLICT ... DO NOTHING` to easily achieve idempotency.

### Migrations are Immutable

Another important principle is that you can't change previously issued instructions.

> Say you created a table, deployed, and now want to add a new field.

You simply EXTEND your implicit migration by appending an `ALTER TABLE` statement to it. And that's the same approach you would use in much more complex migrations setups.

---

## NodeJS Backend App

You can use the `apps/auth` as codebase reference for this section.

### Files Structure

- `src/` - [refer to a ForrestJS App structure](https://forrestjs.github.io/howto/my-first-rest-api/)
  - `services/`
  - `features/`
  - `index.js` - App's entry point
- `test/`
  - `cases/` - E2E test cases
  - `templates/` - Jest templates configuration
- `jest.config.js` - Jest main configuration
- `package.json` - List dependencies and [NPM Scripts](https://docs.npmjs.com/cli/v8/using-npm/scripts)

### Make & DockerCompose Interface

The NodeJS App offers a dockerized Development Environment that:

- runs the App and exposes the port
- monitors the `/src` folder and restard the process
- runs Unit and E2E tests

The same Make APIs are replicated suffixed by the App's name:

```bash
make start-auth
make stop-auth
make test-auth
make build-auth
```

The App provides a `Dockerfile.dev` that builds the Development Container with the NPM dependencies.

**ðŸš§ NOTE:** when you add a new NPM dependency by changing the `package.json` file, you must rebuild the Development Container by running `make build-auth`.

### NPM Interface

Running the App via NPM may not be that easy and it doesn't provide much of an advantage.

The interface is the classic:

```bash
cd services/auth
npm install
npm start
```

But you must provide all the relevant environmental variables that are documented in the `docker-compose.yml`.

### TDD & Native Tests

Running tests is a different matter and it's as easy as:

```bash
cd services/auth
npm install

# Run both Unit and E2E:
npm test

# Start a TDD session:
npm run tdd:unit
npm run tdd:e2e
```

ðŸ‘‰ A TDD session will run the tests and monitor for changes in the soure files, then run it again.

It is a very efficient way of work as you never have to really move your hands away from your keyboard.

> I usually keep 2 Terminal sessions as a side-panel in VSCode
> and run both Unit and E2E TDD.

The E2E tests must communicate with the running App to perform tests via API calls. Hence they need to know WHERE to reach for the App.

The **DEFAULT APP PORT** is hard-coded in `test/templates/e2e/jest.env.js` but you can easily configure it by providing an environmental variable `TEST_URL` via CLI or `.env.development` file.

ðŸ‘‰ Environmental files are sourced by the Test Runner using [dotenv](https://www.npmjs.com/package/dotenv).

---

## Client App in React

We chose [React](https://reactjs.org/) as client rendering engine.

### WebApp vs SPA

Disambguation:

- A SPA is served by a CDN, every service call is [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
- A WebApp is served by WebServer, it can make [same-origin](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy) calls, and _CORS_

**ðŸ‘‰ WE DO WEB APPs ðŸ‘ˆ**

Our goal is to **delegate the security responsibility to the browser** as much as possibile. This mostly goes around the simple trick of saving sensible informations in [_http-only cookies_](<https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#:~:text=A%20cookie%20with%20the%20HttpOnly,cross%2Dsite%20scripting%20(XSS)%20attacks.>) and make server request to the _same-origin_ only.

ðŸš§ We may ask every end-leaf App to build proxy services towards others backend services so to leverage service-to-service [Zero Trust](https://en.wikipedia.org/wiki/Zero_trust_security_model) security principles.

### CRA

We suggest to base our client apps on [Create React App](https://reactjs.org/docs/create-a-new-react-app.html#create-react-app) that is truly simple and removes most of the infrastructural hassles of building a modern web app.

**ðŸ‘‰ DO NOT USE NEXT.JS ðŸ‘ˆ**

Next.js is a wonderful tool but its goal is to build _Server Side Rendered_ apps. We don't need that. Our Apps work **behind login** and do not need to be indexed by search engines.

ðŸ”¥ It is a terrible mistake to enable SSR in our Apps as we would take in-house the rendering costs that are naturally sustained by the customer!!!

### TS Libraries & Components

[[TO BE COMPLETED]]

### Service Calls

[[TO BE COMPLETED]]

### Webpack Proxy

In order to perform _same-origin_ calls you can easily setup a proxy within the basic CRA setup:

https://create-react-app.dev/docs/proxying-api-requests-in-development/

### Client Routing

[[TO BE COMPLETED]]
